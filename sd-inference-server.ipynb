{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whoxg9A87Zl7"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/arenatemp/sd-inference-server.git\n",
        "%cd /content/sd-inference-server\n",
        "!pip install -r requirements.txt\n",
        "!pip install -U xformers\n",
        "!pip install -q --pre triton"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPzx5jKw9OW8"
      },
      "outputs": [],
      "source": [
        "!gdown \"11In-0OXheoKEzl4WrEWK7fRfcGSatpTI&confirm=t\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqub3etfBF-d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import PIL.Image\n",
        "import io\n",
        "import bson\n",
        "import websocket as ws_client\n",
        "import bson\n",
        "import attention\n",
        "import storage\n",
        "import wrapper\n",
        "from server import Server\n",
        "\n",
        "attention.use_optimized_attention()\n",
        "\n",
        "model_storage = storage.ModelStorage(\"./models\", torch.float16, torch.float32)\n",
        "params = wrapper.GenerationParameters(model_storage, torch.device(\"cuda\"))\n",
        "\n",
        "server = Server(params, \"127.0.0.1\", \"28888\")\n",
        "server.start()\n",
        "\n",
        "client = ws_client.WebSocket()\n",
        "client.connect(\"ws://127.0.0.1:28888\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "request = {\"type\":\"txt2img\", \"data\": {\n",
        "    \"model\":\"Anything-V3\", \"sampler\":\"Euler a\", \"clip_skip\":2,\n",
        "    \"prompt\":\"masterpiece, highly detailed, white hair, smug, 1girl, holding big cat\",\n",
        "    \"negative_prompt\":\"bad\", \"width\":384, \"height\":384, \"seed\":2769446625, \"steps\":20, \"scale\":7,\n",
        "    \"hr_factor\":2.0, \"hr_strength\":0.7, \"hr_steps\":20\n",
        "}}\n",
        "client.send_binary(bson.dumps(request))\n",
        "\n",
        "image = None\n",
        "while not image:\n",
        "    response = client.recv()\n",
        "    response = bson.loads(response)\n",
        "    if response[\"type\"] == \"result\":\n",
        "        for i, image_data in enumerate(response[\"data\"][\"images\"]):\n",
        "            image = PIL.Image.open(io.BytesIO(image_data))\n",
        "        response[\"data\"] = \"...\"\n",
        "    print(response)\n",
        "    if response[\"type\"] == \"error\":\n",
        "        break\n",
        "\n",
        "if image:\n",
        "    display(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "client.close()\n",
        "server.stop()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.9 (main, Dec 19 2022, 17:35:49) [GCC 12.2.0]"
    },
    "vscode": {
      "interpreter": {
        "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
